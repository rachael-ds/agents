{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Agentic Workflow for your BigQuery data with LangGraph and Gemini\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook shows you how to build an agent-powered workflow with LangGraph to query structured data using natural language.  This example uses BigQuery, but the approach is easily adaptable to other data sources.\n",
    "\n",
    "[Check out the accompanying blog for more context.](https://medium.com/p/947d0a951a45)\n",
    "\n",
    "The code in this notebook is also used as a base to explore evaluating agentic-workflows and for creating multi-agent workflows: *link*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Set Up\n",
    "\n",
    "#### Install and import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#%%capture --no-stderr\n",
    "#%pip install -U langchain_google_vertexai langgraph google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Annotated, Dict, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Google Cloud (only required if you want to use BigQuery and Gemini)\n",
    "\n",
    "Authenticate for Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#from google.colab import auth\n",
    "#auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate for local Jupyter instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your Google Cloud Project ID, BigQuery Dataset ID and Google Cloud Region - this is only required only if you want to use BigQuery as your data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"\n",
    "DATASET_NAME = \"choc_ai_test\"\n",
    "DATASET_ID = f\"{PROJECT_ID}.{DATASET_NAME}\"\n",
    "REGION = \"US\" \n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload sample data from GCS to BigQuery. \n",
    "We will use a subset of the [chocolate-ai dataset](https://github.com/GoogleCloudPlatform/chocolate-ai), which has information on customer orders from the ficticious  chocolate store. The sample data in this project is hosted in a public Google Cloud Storage bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "dataset = bigquery.Dataset(DATASET_ID)\n",
    "dataset.location = REGION\n",
    "client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "tables = [\"menu\", \"order\", \"order_item\"]\n",
    "gcs_uri = \"choc_ai_dataset\"\n",
    "\n",
    "for table in tables:\n",
    "    table_id = f\"{DATASET_ID}.{table}\"\n",
    "    uri = f\"gs://{gcs_uri}/{table}.csv\"\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_id)  # Check if table exists\n",
    "        print(f\"Table {table_id} already exists. Skipping load.\")\n",
    "        continue  # Skip to the next table\n",
    "    \n",
    "    except NotFound:\n",
    "        load_job = client.load_table_from_uri(\n",
    "            uri, table_id, job_config=bigquery.LoadJobConfig(\n",
    "            autodetect=True, source_format=bigquery.SourceFormat.CSV\n",
    "        ))    \n",
    "        load_job.result() \n",
    "        destination_table = client.get_table(table_id)\n",
    "        print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions\n",
    "\n",
    "These helper functions will be utilized as part of the workflow, but aren't explicitly represented as Tools or Nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_schema() -> str: \n",
    "    \"\"\"Retrieves and caches the BigQuery schema for the specified dataset.\"\"\"\n",
    " \n",
    "    client = bigquery.Client()\n",
    "    schemas = []\n",
    "    tables = client.list_tables(DATASET_ID)\n",
    "    \n",
    "    for table in tables:      \n",
    "        table_id = f\"{table.project}.{table.dataset_id}.{table.table_id}\" \n",
    "        table_obj = client.get_table(table_id)\n",
    "        schema = [field.to_api_repr() for field in table_obj.schema]\n",
    "        schemas.append({\"table_name\": table_id, \"schema\": schema})\n",
    "\n",
    "    return json.dumps({\"tables\": schemas}, indent=2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your Tools\n",
    "\n",
    "Define tools that the agent can use to complete its objective.\n",
    "\n",
    "`execute_query_tool` - in a production environment, you'd want to think about using parameterized queries, or assigning read-only permissions for the Agent to prevent things like SQL injection or unintended writes to your data store.\n",
    "\n",
    "A note on `SubmitFinalAnswer` - this is a tool that is called if the workflow completes. It represents the end state. It's not strictly necessary, but it has a few benefits, especially for more complex workflows or those with multiple agents - it standardizes output and provides a clear signal that the workflow is complete (preventing the agent from getting stuck in a loop). It also makes evaluation easier as you can easily identify the final answer produced by the agent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def execute_query_tool(query: str) -> str:\n",
    "    \"\"\"Execute a SQL query against BigQuery and return the results as a JSON string.\"\"\"\n",
    "  \n",
    "    client = bigquery.Client()\n",
    "    try:\n",
    "      result = client.query_and_wait(query)\n",
    "      r = [dict(row) for row in result]\n",
    "      return str(json.dumps(str(r)))\n",
    "    except Exception as e:\n",
    "      error_message = f\"BigQuery Error: {str(e)}\"\n",
    "      return json.dumps({\"BigQuery error\": error_message})\n",
    "\n",
    "class SubmitFinalAnswer(BaseModel):\n",
    "    \"\"\"Represents the final answer submitted by the agent.\"\"\"\n",
    "\n",
    "    final_answer: str = Field(..., description=\"The final answer to submit to the user\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize your LLMs and assign them tools\n",
    "\n",
    "Create a base LLM object using the `ChatVertexAI` class with the Gemini model. Bind the tools defined above to the LLM so that the LLM can use them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "MODEL = \"gemini-1.5-flash\"\n",
    "\n",
    "data_llm_with_tools = ChatVertexAI(model=MODEL).bind_tools([execute_query_tool, SubmitFinalAnswer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory and State\n",
    "\n",
    "The `State` consists of the schema of the graph (not to be confused with the dataset_schemas below) as well as reducer functions which specify how to apply updates to the state. It represents the current snapshot of your application. The state is passed as an argument to each node in your LangGraph workflow, allowing nodes to read from and update the state as needed. This enables different parts of your workflow to share information and coordinate their actions.\n",
    "\n",
    "The `add_messages` reducer function in our State will append an LLM's response messages to whatever messages are already in the state (rather than overwriting them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"Defines the LangGraph workflow state.\"\"\"\n",
    "\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    dataset_schema: Optional[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State is considered short-term memory and is scoped to a particular thread (a specific instance of your LangGraph workflow - like a single user interaction).\n",
    "\n",
    "\n",
    "Our chatbot can now use tools to answer user questions, but it doesn't remember the context of previous interactions, or graph runs.\n",
    "\n",
    "If you provide a checkpointer when compiling the graph and a thread_id when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same thread_id, the graph loads its saved state, allowing the chatbot to pick up where it left off. This is known as persistent checkpointing.\n",
    "\n",
    "Long-term memory is shared across conversational threads. It can be recalled at any time and in any thread...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nodes for your graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nodes are the fundamental building blocks of a LangGraph workflow. They represent individual units of work or processing steps. Each node is a function that takes the current state dictionary as input, performs its task, and returns a modified state dictionary. This modified state is then passed to the next node in the workflow based on the connections defined in the graph.\n",
    "This is the basic pattern for all LangGraph node functions.\n",
    "\n",
    "`get_schema_node`: This node retrieves and stores the database schema in the state if it's not already present. This ensures the schema is fetched only once.\n",
    "\n",
    "`data_chatbot_node`: This node uses an LLM to understand user requests (the prompt). Guided by its system instructions and the schema, it will generate a SQL query and use the tools at its disposal to execute this query in BigQuery.\n",
    "\n",
    "`get_state`: This node controls the workflow's transitions. It checks the last message in the conversation to decide whether to call a tool, the data_chatbot node, or end the workflow altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sys_message = \"\"\"You are an expert in answering questions users have about their data stored in BigQuery. You job is to execute the relevant SQL statements against BigQuery tables to get the best answer. The user is only interested in seeing the final result from BigQuery.\n",
    "\n",
    "1. If the user request is reasonable and compatible with the schema, YOU MUST FIRST call the `execute_query_tool`to get the result.\n",
    "    When generating the SQL query:\n",
    "    Use meaningful aliases for column names.    \n",
    "    Limit results to 5 rows (unless specified). Order results for clarity.\n",
    "    Select only necessary columns; avoid SELECT *.\n",
    "    Use valid BigQuery SQL (no escape characters).\n",
    "    Use only SELECT statements (no DML).    \n",
    "\n",
    "2. Call the `execute_query_tool` tool to execute the generated SQL query. If the query fails, analyze the error message and attempt to correct the SQL.  If correction is not possible, inform the user of the error and its likely cause.\n",
    "\n",
    "3. Only once you have the result from BigQuery, call the `SubmitFinalAnswer` tool to present the final results to the user and terminate the conversation. Do not call any other tools after this.\n",
    "\n",
    "You will use the following schema for all queries and all SQL must conform to this schema: {schema}\n",
    "\n",
    "**Example:**\n",
    "If a user asks: 'which is the most expensive item on the menu?' you should:\n",
    "1. Call the execute_query_tool to execute SQL: 'SELECT menu_name, menu_price as price FROM `dataset_name.choc_ai_test.menu` ORDER BY menu_price DESC LIMIT 1'\n",
    "2. Only then can you call SubmitFinalAnswer tool to respond to the user with the result of this query in natural language and end the conversation.\n",
    "3. Once the SubmitFinalAnswer tool has been called, you must ALWAYS end the workflow. Do not call any other tools. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_schema_node(state: Dict) -> Dict:\n",
    "    \"\"\"Retrieves the schema from BigQuery and stores it in the state.\"\"\"\n",
    "    \n",
    "    if state.get(\"dataset_schema\") is None: \n",
    "        schema = get_schema()  \n",
    "        return {\"dataset_schema\": schema, \"messages\": [AIMessage(content=\"Schema retrieved from BigQuery\")]}\n",
    "    return {\"messages\": [AIMessage(content=\"Schema retrieved from memory\")]}\n",
    "\n",
    "def data_chatbot_node(state: Dict) -> Dict:\n",
    "    \"\"\"Extracts user requirements using an LLM.\"\"\"\n",
    "    \n",
    "    schema = state[\"dataset_schema\"]        \n",
    "    messages = [SystemMessage(content=sys_message.format(schema=schema))] + state[\"messages\"]\n",
    "    response = data_llm_with_tools.invoke(messages)    \n",
    "    return {\"messages\": [response]}\n",
    "    \n",
    "def get_state(state: Dict) -> str:\n",
    "    \"\"\"Determine the next steps in the workflow.\"\"\"\n",
    "    \n",
    "    last_message = state[\"messages\"][-1]     \n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        if any(tool_call[\"name\"] == \"execute_query_tool\" for tool_call in last_message.tool_calls):\n",
    "            return \"execute_sql\" \n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your graph\n",
    "\n",
    "Define the structure of the agent's workflow using `StateGraph(State)` - this represents the workflow as a directed graph.\n",
    "\n",
    "Next, add the nodes defined above to the StateGraph, these are like individual steps in your workflow. \n",
    "The `START` node specifies the entry point - this tells our graph where to start its work each time we run it.\n",
    "Similarly, the `END` node instructs the graph the end the workflow when this node is run. When the workflow reaches the `END` node, it means that the user's input has been processed, and they are now free to provide another.\n",
    "\n",
    "You can then add your edges, which define the order of these steps. Conditional edges execute functions to determine which node is next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"get_schema\", get_schema_node)\n",
    "workflow.add_node(\"data_chatbot\", data_chatbot_node)\n",
    "workflow.add_node(\"execute_sql\", ToolNode([execute_query_tool]))\n",
    "\n",
    "workflow.add_edge(START, \"get_schema\")\n",
    "workflow.add_edge(\"get_schema\", \"data_chatbot\")\n",
    "workflow.add_conditional_edges(\"data_chatbot\", get_state, [\"execute_sql\", END]) \n",
    "workflow.add_edge(\"execute_sql\", \"data_chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the workflow into a runnable graph and then visualize its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_chatbot_graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(data_chatbot_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go agents, go!\n",
    "\n",
    "Finally, run the graph! \n",
    "\n",
    "The config dictionary includes:\n",
    "-   `thread_id`  if you have provided a checkpointer at compilation time like we have, the thread_id will save the state in memory after each step. When you invoke the graph again using the same thread_id, the graph loads its saved state, allowing the chatbot to pick up where it left off.\n",
    "- `recursion_limit`  sets the number of supersteps that the graph is allowed to execute before it raises an error.\n",
    "\n",
    "The `data_chatbot_graph.stream` method streams back outputs from a graph run. You can stream a graph in different modes (values, updates, debug) depending on how much information you want returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"101\"}, \"recursion_limit\": 20}\n",
    "\n",
    "while True:    \n",
    "    user = input(\"User (q/Q to quit): \")\n",
    "\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        break\n",
    "    output = None\n",
    "    for output in data_chatbot_graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
